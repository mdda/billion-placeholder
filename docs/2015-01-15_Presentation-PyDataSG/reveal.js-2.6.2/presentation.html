<!doctype html>
<html lang="en">

 <head>
  <meta charset="utf-8">

  <title>Theano for the Billion Word Kaggle</title>

  <meta name="description" content="Presentation on Theano for the PySG MeetUp">
  <meta name="author" content="Martin Andrews">

  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

  <link rel="stylesheet" href="css/reveal.min.css">
  <link rel="stylesheet" xhref="css/theme/default.css" href="css/theme/sky.css" id="theme">

  <!-- For syntax highlighting -->
  <link rel="stylesheet" href="lib/css/zenburn.css">

  <!-- If the query includes 'print-pdf', include the PDF print sheet -->
  <script>
   if( window.location.search.match( /print-pdf/gi ) ) {
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = 'css/print/pdf.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
   }
  </script>

  <!--[if lt IE 9]>
  <script src="lib/js/html5shiv.js"></script>
  <![endif]-->
 </head>

 <body>
  <div class="reveal">
   <!-- Any section element inside of this container is displayed as a slide -->
   <div class="slides">
    
<style>
table.table-fix {
 margin-left:auto;  margin-right:auto; border-collapse:collapse; cell-padding:5px;
 margin-top:20px;
}
.table-fix td,.table-fix th {
 padding: 6px;
}
.table-fix th {
 border-bottom:1pt solid black;
}
.fix-spacing li {
 margin-bottom:16pt;
}
</style>

<!--
## Target : 10-20 mins

Intro : Motivation

Explain Kaggle competition
  Test set
    100k sentences each with 1 word missing
    Levenshtein distance (character errors)
  Training set
    1Bn word Corpus 
  "State of Play"
    End date
    Lack of entries

Explain Approach
  WordEmbedding
    How this is exciting
  Getting the vectors
    word2vec
    GloVE

Explain Theano
  Python module that describes relationships
    Implements through numpy and GPU
    GPU : CUDA and OpenCL routes
  Simple idea
    Code example  
  Show updates
    Symbolic differentiation 'for free'
    Code example
  'lasagne' deep learning setup too
    Code example
  
Explain Theano Implementation
  GPU consideration : Bus bandwidth
  Need to do Word Embedding trick on-GPU
  Paging in of training set

Status
  What exists
    Already learning gap-detection
  Next Steps
    Maybe also do fill-in model
    Open Source in ~ 1 week

End
!-->

<section>
 <h1>1Bn Word NLP</h1>
 <h3>with Theano</h3>
 <p>
  <small><a href="http://mdda.net">Martin Andrews</a> / <a href="http://twitter.com/redcatlabs">@redcatlabs</a></small>
 </p>
 <p>
  <small>15 January 2015</small>
 </p>
</section>



<section>
  <section>
   <h2>Background</h2>
   <ul class="fix-spacing">
    <li>Finance / Startups / Machine Learning</li>
    <li>Moved to Singapore in Sep-2013</li>
    <li>The past year (2014) = 'fun' :</li>
    <li style="list-style-type:none">
      <ul>
        <li>Machine Learning Experimentation : Deep Learning, NLP</li>
        <li>Languages : Python, Go, Scala, NodeJS, Haskell, Python</li>
        <li>"MeetUp Pro" / Kaggle Novice</li>
      </ul>
    </li>
   </ul>
  </section>
  <section>
   <h2>Motivation</h2>
   <ul class="fix-spacing">
    <li>Goals :</li>
    <li style="list-style-type:none">
      <ul>
        <li>Do some real-world-scale NLP</li>
        <li>Use Theano</li>
        <li>Improve Theano (OpenCL)</li>
      </ul>
    </li>
    <li>Magic Solution : Kaggle Competition</li>
   </ul>
  </section>
</section>

<section>
  <section>
   <h2>1Bn Word Imputation</h2>
   <h3>Kaggle Competition</h3>
   <a href="http://www.kaggle.com/c/billion-word-imputation" target=_blank>
     <img width="800" height="284" src="img/KaggleScreenshot_800x284.png" alt="Kaggle Screenshot">
   </a>
  </section>
  <section>
   <h2>Test Set</h2>
   <ul class="fix-spacing">
    <li>~300k sentences, each with 1 word missing</li>
    <li style="list-style-type:none">
      <ul>
        <li>Want : Each sentence filled-in correctly</li>
      </ul>
    </li>
    <li>Scoring : </li>
    <li style="list-style-type:none">
      <ul>
        <li>Levenshtein distance = character-based errors</li>
        <li>This is a <i>linguistically odd</i> objective function</li>
      </ul>
    </li>
   </ul>
  </section>
  <section>
   <h2>Training Set</h2>
   <ul class="fix-spacing">
    <li>"1 billion" word corpus :</li>
    <li style="list-style-type:none">
      <ul>
        <li>769 million words = 4.1Gb of data</li>
        <li>30 million sentences</li>
        <li>Drawn from many different sources</li>
      </ul>
    </li>
    <li>Only knowledge derived from this data is allowed</li>
   </ul>
  </section>
  <section>
   <h2>Kaggle Status</h2>
   <ul class="fix-spacing">
    <li>Competition ends in May-2015</li>
    <li>But activity has mostly died down : </li>
    <li style="list-style-type:none">
      <ul>
        <li>Impressive groups with CPU clusters</li>
        <li>Data size is also a hurdle</li>
        <li>Only for fun : No 'points'</li>
      </ul>
    </li>
    <li>Will release 'Getting Started' Code for laptop</li>
   </ul>
  </section>
</section>

<section>
  <section>
   <h2>My Basic Approach</h2>
   <ul class="fix-spacing">
    <li>Find where there's a word missing</li>
    <li>For that position think up a word</li>
    <li>Apply Deep Learning to score possibilities</li>
   </ul>
   <img width="700" height="131" src="img/img-to-cat_700x131.png" alt="Image to Cat" style="border:none;box-shadow:none">
  </section>
  <section>
   <h2>Word Embedding</h2>
   <p>How to handle 'discrete' things like Words?</p>
   <img width="732" height="360" src="img/word-embedding_732x360.png" alt="Word Embedding" style="border:none;box-shadow:none">
  </section>
  <section>
   <h2>Trainable on Wikipedia</h2>
   <img width="700" height="267" src="img/embedding-training_700x267.png" alt="Word Embedding Training" style="border:none;box-shadow:none">
   <p>5.7MM documents, 5.4Bn terms<br />&rarr; 155k words, 500-D embedding</p>
  </section>
  <section>
   <h2>Finds Relationships</h2>
   <img width="446" height="360" src="img/facts_446x360.png" alt="Word Embedding Facts" style="border:none;box-shadow:none">
   <p>Ready-made Python module : Word2Vec</p>
  </section>
  <section>
   <h2>Semantic too</h2>
   <img width="535" height="289" src="img/grammar_535x289.png" alt="Word Embedding Grammar" style="border:none;box-shadow:none">
   <p>This is pretty surprising, IMHO</p>
  </section>
</section>

<section>
  <section>
   <h2>Theano</h2>
   <ul class="fix-spacing">
    <li>Optimised Numerical Computation in Python</li>
    <li>Works in conjunction with <code>numpy</code></li>
    <li>Computation is 'described' in Python code: </li>
    <li style="list-style-type:none">
      <ul>
        <li>Theano operates on expression tree itself</li>
        <li>Optimizes the tree for operations it knows</li>
        <li>Spits out code in <code>C/C++</code> or <code>CUDA</code> (or <code>OpenCL</code>)</li>
      </ul>
    </li>
   </ul>
  </section>
  <section>
   <h2>Theano : Basic</h2>
   <p>Function 'built up', then evaluated</p>
   <pre><code data-trim contenteditable>
import theano.tensor as T
x = T.matrix("x") # Declare Theano symbolic variables
y = T.vector("y")
w = theano.shared(rng.randn(feats), name="w")
b = theano.shared(0., name="b")

# Construct Theano expression graph
p_1 = 1 / (1 + T.exp(-T.dot(x, w) - b))   # Probability that target = 1
prediction = p_1 > 0.5                    # The prediction thresholded

predict = theano.function(inputs=[x], outputs=prediction)

print predict( [0.1, .02, ... , -7.4, 3.2] )
   </code></pre>
  </section>
  <section>
   <h2>Theano : Iterative</h2>
   <p>Gradients come 'free'</p>
   <pre><code data-trim contenteditable>
xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) # Cross-entropy loss fn
cost = xent.mean() + 0.01 * (w ** 2).sum()    # Minimize this

gw, gb = T.grad(cost, [w, b])  # Compute the gradient of the cost

train = theano.function( 
          inputs=[x,y],
          outputs=[prediction, xent],
          updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb)))

for i in xrange(training_steps):
    pred, err = train(data_X, data_y)   
   </code></pre>
   <p>Iteration mechanism built-in</p>
  </section>
  <section>
   <h2>Theano : Lasagne</h2>
   <p>Thin 'NN' layer on top of regular Theano</p>
   <pre><code data-trim contenteditable>
l_in = lasagne.layers.InputLayer(
    shape=(batch_size, 1, input_width, input_height) )
    
l_conv2 = cuda_convnet.Conv2DCCLayer( l_in,
    num_filters=32, filter_size=(5, 5),
    nonlinearity=lasagne.nonlinearities.rectify )
    
l_pool2 = cuda_convnet.MaxPool2DCCLayer( l_conv2, ds=(2, 2))

l_hidden = lasagne.layers.DenseLayer( l_pool2, num_units=256,
    nonlinearity=lasagne.nonlinearities.rectify )

l_dropout = lasagne.layers.DropoutLayer(l_hidden, p=0.5)

l_out = lasagne.layers.DenseLayer( l_dropout, num_units=output_dim,
    nonlinearity=lasagne.nonlinearities.softmax )
   </code></pre>
  </section>
</section>


<section>
  <section>
   <h2>1Bn Words ...</h2>
   <ul class="fix-spacing">
    <li>Size of DataSet is a problem</li>
    <li>Word-Embedding is 'new'</li>
    <li>Playing around with 'gaps' objective</li>
   </ul>
  </section>
  <section>
   <h2>DataSet size</h2>
   <ul class="fix-spacing">
    <li>Reduce training set to manageable size during dev.</li>
    <li>Even so : 1MM sentences &rArr; 70MM training examples</li>
    <li>Will eventually need to page data into GPU</li>
    <li>Need to consider PCI bus bandwidth (epochs, etc)</li>
   </ul>
  </section>
  <section>
   <h2>Word Embedding</h2>
   <ul class="fix-spacing">
    <li>vocab.size &gt; 65k &rArr; word[&middot;] : int32</li>
    <li>Training examples :</li>
    <li style="list-style-type:none">
     <ul>
      <li>( word[i-1], word[i] ) &rarr; ( gap_type=0 )</li>
      <li>( word[i-1], word[i+1] ) &rarr; ( gap_type>0 )</li>
     </ul>
    </li>
    <li>Store 240-D vectors on GPU (150Mb)</li>
    <li>Expand word-context to 480-D input vector</li>
   </ul>
  </section>
  <section>
   <h2>Gaps objective</h2>
   <ul class="fix-spacing">
    <li><code>gap_type</code> is softmax over 2+32 classes : </li>
    <li style="list-style-type:none">
      <ul>
        <li>{ NoGap, ComplexGap } ++ </li>
        <li>{ the , . to of a and in {N} " that 's for on is was with ... }</li>
      </ul>
    </li>
    <li>Base case for <code>gap.best</code> &rArr; add a space</li>
    <li>Filling known gap with uncertain word &rArr; Risky</li>
   </ul>
   <ximg width="700" height="131" src="img/img-to-cat_700x131.png" alt="Image to Cat" style="border:none;box-shadow:none">
  </section>
</section>

<section>
  <section>
   <h2>Wrap-up</h2>
   <ul class="fix-spacing">
    <li>Kaggle Competitions are Cool</li>
    <li>Word Embedding is Surprising</li>
    <li>Theano makes GPUs Python-friendly</li>
    <li><em>Will "Formally Announce" ASAP</em></li>
    <li style="list-style-type:none">
      <ul>
        <li><em>(all code already on GitHub)</em></li>
      </ul>
    </li>
   </ul>
  </section>
  <section>
   <h2>The Future</h2>
   <ul class="fix-spacing">
    <li>This coming year (2015) = 'serious' :</li>
    <li style="list-style-type:none">
      <ul>
        <li>Working for Local Company</li>
        <li>Sole Focus : NLP (financial documents &rarr; relationships)</li>
      </ul>
    </li>
   </ul>
  </section>
</section>


<section>
 <h1>- QUESTIONS -</h1>
 <br>
 <h3>Martin.Andrews @<br> RedCatLabs.com</h3>
 <br>
 <p>( <a href="https://github.com/mdda">'mdda' on GitHub!</a> )</p>
</section>

   </div>
  </div>

<div id="redcatlabs-logo" style="background: url(img/redcatlabs_logo1_280x39.png);
                                  position: absolute;
                                  bottom: 50px;
                                  left: 50px;
                                  width: 280px;
                                  height: 39px;">
</div>  

  <script src="lib/js/head.min.js"></script>
  <script src="js/reveal.min.js"></script>

  <script>

   // Full list of configuration options available here:
   // https://github.com/hakimel/reveal.js#configuration
   Reveal.initialize({
    controls: true,
    progress: true,
    history: true,
    center: true,

    theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
    transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

    // Parallax scrolling
    // parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
    // parallaxBackgroundSize: '2100px 900px',

    // Optional libraries used to extend on reveal.js
    dependencies: [
     { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
     { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
     { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
     { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
     { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
     { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
    ]
   });

  </script>

 </body>
</html>
