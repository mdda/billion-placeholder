## Notes on ipython notebook install :: 
#  http://blog.mdda.net/oss/2014/10/13/ipython-on-fedora/

ADD : 
yum install gcc-c++
## But:: gcc versions 4.9 and up are not supported...
# See Fix on :: https://devtalk.nvidia.com/default/topic/793839/ubuntu-14-04-1-lts-cuda-6-5-gcc-4-9/
# On Fedora 21, requires commenting out lines 77-87 of /usr/local/cuda-6.5/include/host_config.h

## Also :: Why is this compiling for CUDA without being asked?

. env/bin/activate

##pip install python-Levenshtein
pip install editdistance
pip install hickle



cd env
git clone https://github.com/benanne/Lasagne.git
cd Lasagne
python setup.py develop


GloVe from :: http://www-nlp.stanford.edu/projects/glove/

wget http://www-nlp.stanford.edu/software/glove.tar.gz
tar -xzf glove.tar.gz
cd glove
make 
# .. that's it ...

# run ./demo.sh for fun...
# 100Mb file, 17M words in sentences, 253k vocab 
# Actual training : 81k word vocab, vector-length : 50, max_iter 15
15:30 - 15:42 (finished earlier...)
23:48 - 23:55 (started earlier...)

wc data/orig/train_v2.txt 
  30,301,028  768,646,526 4,147,291,308 data/orig/train_v2.txt
wc data/orig/test_v2.txt
  306,682  7,481,193 43,045,391 data/orig/test_v2.txt

# The GloVe sample 'text8' corpus file appears to be just :
#   plain lowercase words
#   no punctuation
#   no line-gaps
#   numbers spelled out as digits
#   apostrophes->spaces

mkdir -p data/{0-orig,1-holdout,2-glove,3-gaps,4-fill}

## Move the Kaggle training and test data into the right place on the faster machine
#rsync -avz --progress data/0-orig andrewsm@holland.herald:/home/andrewsm/sketchpad/kaggle/1-billion-words/data/



## Create a 1-holdout/{train,heldout} set, so we can benchmark ourselves locally 
## This includes a synthetic truth/valid set, using the heldout data
python src/make_holdout_files.py --input data/0-orig/train_v2.txt \
								 --train data/1-holdout/train.txt --heldout data/1-holdout/heldout.txt \
                 --truth data/1-holdout/truth.txt --valid data/1-holdout/valid.txt 

## Test to see what the holdout test sample scores vs its own source data (synthetic data from above)
python src/score_vs_holdout.py --orig data/1-holdout/truth.txt --submission data/1-holdout/valid.txt 
# End result is 71627 cases : 5.54351 distance 1SD=(5.53332, 5.55372) (ideal = 5.55211)



## Create preprocessed corpus files that can be fed to GloVe directly

##  1MM 19s (laptop), 1MM 10s (holland)
#python src/make_corpus.py --input data/1-holdout/train.txt --output data/2-glove/1MM_0-corpus.txt --lines 1000000
#python src/make_corpus.py --input data/1-holdout/heldout.txt --output data/2-glove/1MM_4-heldout.txt --lines 1000000

##   ALL 10m03s (laptop),  ALL 5m50s (holland) :: 20% of words have >=100 occurrences (heldout file is same as 1MM)
#python src/make_corpus.py --input data/1-holdout/train.txt --output data/2-glove/ALL_0-corpus.txt
#python src/make_corpus.py --input data/1-holdout/heldout.txt --output data/2-glove/ALL_4-heldout.txt


## Now run GloVE (on a couple of machine to see the speed difference) :
## Vector size is 240 floats

# 0.0457 -> 0.0123 in 12:45m (holland), 25m (laptop) - 61MM contexts(?), 83k vocab
./src/corpus-glove.sh 1MM 

# 0.0287 -> 0.0083 in 3h45m (holland, HD) 3h42m (holland, SSD) 768MM words, 2.14MM unique words, vocab(>=5)=511k words
# "TRAINING MODEL" stage (only) uses all cores of machine
./src/corpus-glove.sh ALL


## Move pre-computed vectors back to laptop
rsync -avz --progress andrewsm@holland.herald:~/OpenSource/1-billion-words/data/2-glove/ALL_1-vocab.txt data/2-glove/
rsync -avz --progress andrewsm@holland.herald:~/OpenSource/1-billion-words/data/2-glove/ALL_3-vectors.txt data/2-glove/

python src/glove_to_hickle.py --input data/2-glove/1MM_3-vectors.txt \
                              --output data/2-glove/1MM_3-vectors.hickle --size 240
python src/glove_to_hickle.py --input data/2-glove/ALL_3-vectors.txt \
                              --output data/2-glove/ALL_3-vectors.hickle --size 240


## NO : Don't do this - there's no need for intermediate file to be stored...
## Derive the 'gap' training data from the corpus (choose which one according to taste/time...)
## 1MM takes <4mins
##python src/make_gaps_training.py --input data/2-glove/1MM_0-corpus.txt --vocab data/2-glove/1MM_1-vocab.txt \
##                                 --output data/3-gaps/1MM_train.txt --small 32
##python src/make_gaps_training.py --input data/2-glove/1MM_4-heldout.txt --vocab data/2-glove/1MM_1-vocab.txt \
##                                 --output data/3-gaps/1MM_valid.txt --small 32

# Invocation intended...  (using 1MM throughout)
python src/gap_model.py   --vocab data/2-glove/1MM_1-vocab.txt --vectors data/2-glove/1MM_3-vectors.hickle --small 32 \
						  --mode train \
						  --train data/2-glove/1MM_0-corpus.txt --valid data/2-glove/1MM_4-heldout.txt \
						  --save data/3-gaps/1MM_model.hickle \
              --epochs 2

# Use 'ALL' vector dictionary
python src/gap_model.py   --vocab data/2-glove/ALL_1-vocab.txt --vectors data/2-glove/ALL_3-vectors.hickle --small 32 \
						  --mode train \
						  --train data/2-glove/1MM_0-corpus.txt --valid data/2-glove/1MM_4-heldout.txt \
						  --save data/3-gaps/1MM_model.hickle \
              --epochs 2


## To run on GPU/old-CUDA (benchmark speedup 464 vs 1175, ie: 2.5x faster than CPU)
## Prepend :
THEANORC=theano.cuda-sandbox.rc optirun python ...

# Check ReShape() is as expected : see end of gap_model.py
# Check default scale of initial W(,), b() : W=init.Uniform(), b=init.Constant(0.), which is a little odd

# TODO :
# Check scale of GloVE vectors : N(0, 0.5) at 'top' of vocab, N(0, 0.07) at 'end' of vocab !!! 
Normalize to being N(.,1) throughout
#   See end of gap_model.py - may be a reason to try word2vec, maybe

# Training errors tell a very odd story...
# 1MM vocab gives better results (1 epoch = 63.2%), than ALL vocab (1 epoch = 56.37%)
# Even if ALL vocab is truncated to same length (1 epoch = 55.5%)
# Tried 1MM vocab with different seeds too : No change in good result

# Are both vector files normalized? == NO.  Run normalization on 'ALL' too (and limit to 82k vocab)
#   But that moves 1 epoch to 51.4% !
# Rerun with no vocab cap : 59.6%

run             seed   epoch1   N  epochN
1MM norm               63.15    2  64.44
1MM norm        123    63.20    2  64.41
1MM norm        1234   63.20    2  64.40
1MM norm retry         63.31    2  64.31
# ^^ very consistent (reassuring)

ALL raw  uncap         56.37   10  62.09
ALL norm uncap  1234   59.59    2  61.60
ALL norm uncap  1234   59.02   10  64.93
# ^^  Normalization helps uncapped 'ALL' by 2-3%

ALL raw  limit         55.48    2  55.49
ALL norm limit  1234   51.37    2  53.93
ALL norm limit  123    51.37    2  51.38
ALL norm limit  1235   51.37    2  51.38 # identical!  init: 0.1 (unch)
# ^^ Normalizing the ALL vocab (when truncated) HURTS performance = Puzzling

ALL norm limit  1235   51.37    2  51.37 # init : hidden:0.1*1, output:0.1*0.1
ALL norm limit  1235   51.37    2  51.38 # init : hidden:0.1*1, output:0.1*1
ALL norm limit  1235   51.36    2  54.88 # init : hidden:0.1*1, output:0.1*3
ALL norm limit  1235   51.38    2  54.82 # init : hidden:0.1*3, output:0.1*3
ALL norm limit  1235   51.37    2  55.02 # init : hidden:0.1*3, output:0.1*9
ALL norm limit  1235   51.40    2  55.70 # init : hidden:0.1*9, output:0.1*9

ALL norm limit  1235   51.37    2  51.38 # init : hidden:0.1*9, output:0.3*9 ## Too far...
ALL norm limit  1235   51.35    2  51.36 # init : hidden:0.3*9, output:0.1*9 ## Too far...



# Is the vocab in a different order? 
# 1MM vocab and vectors confirmed to be in same order (=#occurrences, then alphabetic)
# ALL vocab and vectors = same? :: Unfortunately, yes.  @82000, #occurrences = 133



## Why is this?  Ideas:  1MM vocab is precisely what is needed for 1MM training set
##    But : the results apply to validation set too - where vocab is not pre-selected
##    Also : ALL-vocab should be 'better defined', but truncating it is (CHECK) worse, not better
##    As expected, normalisation helps.



save model to disk 

run on GPU/new-CUDA (benchmark speedup)
:: Prepend :
THEANORC=theano.cuda-gpuarray.rc optirun python ...

load model from disk
run test data - and build submission (with / without small words)

gather 'confidence' level for each answer (size of max output) or (ratio of 1st place to 2nd place), etc





python src/gap_model.py   --vocab data/2-glove/1MM_1-vocab.txt --vectors data/2-glove/1MM_3-vectors.hickle --small 32 \
						  --mode test \
						  --load data/3-gaps/1MM_model.hickle \
						  --test data/0-orig/test.txt --output data/3-gaps/1MM_submission.txt


Theano : 
  Theano implementation of SENNA NER network
    https://github.com/Fematich/nn_ner
      (asked about licensing)
    
  (Multi-layer Hidden&ReLu + LogisticOutput) with ADAgrad 
	http://nbviewer.ipython.org/github/dawenl/deep_tagging/blob/master/code/deep_tagging.ipynb
	https://github.com/dawenl/deep_tagging
	  GPL3 licensed
    
  Deep Learning Tutorial : NLP/word-embedding
    http://deeplearning.net/tutorial/rnnslu.html
    https://github.com/mesnilgr/is13/blob/master/rnn/elman.py
      Creative Commons Attribution-NonCommercial 4.0 International License.
    
  GloVE
	https://github.com/maciejkula/glove-python/
	http://radimrehurek.com/2014/12/making-sense-of-word2vec
	  Apache 2

  Similar to Bengio 2003 :
	https://bitbucket.org/kostialopuhin/word-models
	  word_embeddings.py

	
	
  nntools (now Lasagne?) (FF-NN focussed, but no embedding?)
    https://github.com/benanne/Lasagne
      MIT License
   
  blocks : (More RNN-focussed)
    http://blocks.readthedocs.org/en/latest/
      MIT License
    
  
  py2learn :  (Heavily dependent on structure YAML - rather than code - apparently)
	http://nbviewer.ipython.org/github/lisa-lab/pylearn2/blob/master/pylearn2/scripts/tutorials/multilayer_perceptron/multilayer_perceptron.ipynb
	# Dig into :: http://deeplearning.net/software/pylearn2/index.html


	
http://stackoverflow.com/questions/25166657/index-gymnastics-inside-a-theano-function

# @author Jonathan Raiman
# Messing around with Stanford's GloVe words
https://gist.github.com/JonathanRaiman/0d45d1ab214119cf45eb

