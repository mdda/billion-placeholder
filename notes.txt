## Notes on ipython notebook install :: 
#  http://blog.mdda.net/oss/2014/10/13/ipython-on-fedora/

##pip install python-Levenshtein
pip install editdistance

GloVe from :: http://www-nlp.stanford.edu/projects/glove/

wget http://www-nlp.stanford.edu/software/glove.tar.gz
tar -xzf glove.tar.gz
cd glove
make 
# .. that's it ...

# run ./demo.sh for fun...
# 100Mb file, 17M words in sentences, 253k vocab 
# Actual training : 81k word vocab, vector-length : 50, max_iter 15
15:30 - 15:42 (finished earlier...)
23:48 - 23:55 (started earlier...)

wc data/orig/train_v2.txt 
  30,301,028  768,646,526 4,147,291,308 data/orig/train_v2.txt
wc data/orig/test_v2.txt
  306,682  7,481,193 43,045,391 data/orig/test_v2.txt

# The GloVe sample 'text8' corpus file appears to be just :
#   plain lowercase words
#   no punctuation
#   no line-gaps
#   numbers spelled out as digits
#   apostrophes->spaces

mkdir -p data/{0-orig,1-holdout,2-glove,3-gaps,4-fill}

## Move the Kaggle training and test data into the right place on the faster machine
#rsync -avz --progress data/0-orig andrewsm@holland.herald:/home/andrewsm/sketchpad/kaggle/1-billion-words/data/



## Create a 1-holdout/{orig,test} set, so we can benchmark ourselves locally
python src/make_holdout_files.py --input data/0-orig/train_v2.txt --orig data/1-holdout/orig.txt --test data/1-holdout/test.txt 

## Test to see what the holdout test sample scores vs its own source data
python src/score_vs_holdout.py --orig data/1-holdout/orig.txt --submission data/1-holdout/test.txt 
# End result is 71627 cases : 5.54351 distance 1SD=(5.53332, 5.55372) (ideal = 5.55211)



## Create preprocessed corpus files that can be fed to GloVe directly
date
##  1MM 19s (laptop), 1MM 10s (holland)
#python src/make_corpus.py --input data/0-orig/train_v2.txt --output data/2-glove/1MM_0-corpus.txt --lines 1000000
##  10MM 3m25s
#python src/make_corpus.py --input data/0-orig/train_v2.txt --output data/2-glove/10MM_0-corpus.txt --lines 10000000
##   ALL 10m03s (laptop),  ALL 5m50s (holland) :: 20% of words have >=100 occurrences
#python src/make_corpus.py --input data/0-orig/train_v2.txt --output data/2-glove/ALL_0-corpus.txt
date

## Now run GloVE (on a couple of machine to see the speed difference) :
# 0.0457 -> 0.0123 in 12m (holland), 25m (laptop)
./src/corpus-glove.sh 1MM 
# 0.0287 -> 0.0083 in 3h45m (holland)
./src/corpus-glove.sh ALL

## Move pre-computed vectors back to laptop
rsync -avz --progress andrewsm@holland.herald:~/sketchpad/kaggle/1-billion-words/data/2-glove/ALL_1-vocab.txt data/2-glove/
rsync -avz --progress andrewsm@holland.herald:~/sketchpad/kaggle/1-billion-words/data/2-glove/ALL_3-vectors.txt data/2-glove/


## Derive the 'gap' training data from the corpus (choose which one according to taste/time...)
python src/make_gaps_training.py --input data/2-glove/1MM_0-corpus.txt --vocab data/2-glove/1MM_1-vocab.txt --output data/3-gaps/1MM_train.txt 



# Dig into :: http://deeplearning.net/software/pylearn2/index.html







Theano : 
  Theano implementation of SENNA NER network
    https://github.com/Fematich/nn_ner
    
  (Multi-layer Hidden&ReLu + LogisticOutput) with ADAgrad 
	http://nbviewer.ipython.org/github/dawenl/deep_tagging/blob/master/code/deep_tagging.ipynb
	https://github.com/dawenl/deep_tagging
    
  Deep Learning Tutorial : NLP/word-embedding
    http://deeplearning.net/tutorial/rnnslu.html
    https://github.com/mesnilgr/is13/blob/master/rnn/elman.py
    
  GloVE
	https://github.com/maciejkula/glove-python/
	http://radimrehurek.com/2014/12/making-sense-of-word2vec

  Similar to Bengio 2003 :
	https://bitbucket.org/kostialopuhin/word-models
	  word_embeddings.py

	
	
  nntools (now Lasagne?) (FF-NN focussed, but no embedding?)
    https://github.com/benanne/Lasagne
   
  blocks : (More RNN-focussed)
    http://blocks.readthedocs.org/en/latest/
  
  py2learn :  (Heavily dependent on structure YAML - rather than code - apparently)
	http://nbviewer.ipython.org/github/lisa-lab/pylearn2/blob/master/pylearn2/scripts/tutorials/multilayer_perceptron/multilayer_perceptron.ipynb

	
http://stackoverflow.com/questions/25166657/index-gymnastics-inside-a-theano-function

# @author Jonathan Raiman
# Messing around with Stanford's GloVe words
https://gist.github.com/JonathanRaiman/0d45d1ab214119cf45eb



